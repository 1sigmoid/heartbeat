{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing necessary libraries\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Making a list of all the files in the dataset biorxiv_medrxiv directory\n",
    "\"\"\"\n",
    "\n",
    "entries = os.listdir(\"./../../datasets/CORD-19/biorxiv_medrxiv/biorxiv_medrxiv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "initializing a list for storing \n",
    "all the dictionaries made from the json files\n",
    "\"\"\"\n",
    "\n",
    "datals = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creating a string to append the base directory\n",
    "\"\"\"\n",
    "\n",
    "s = \"./../../datasets/CORD-19/biorxiv_medrxiv/biorxiv_medrxiv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "reading the files from the given directory\n",
    "\"\"\"\n",
    "\n",
    "for i in entries:\n",
    "    with open(s+i) as f:\n",
    "        dat = json.load(f)\n",
    "        datals.append(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters:\n",
    "(list) abstractls - storing the abstract text from all the given documents\n",
    "(list) bodyls - storing the body text from all the given docs\n",
    "(list) temp_ls - to store temp data while reading all docs\n",
    "\"\"\"\n",
    "\n",
    "abstractls = [] \n",
    "bodyls = []\n",
    "temp_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datals)): \n",
    "    #sets the working file\n",
    "    current_data = datals[i]\n",
    "    \"\"\"\n",
    "    abstract is a list containing dictionaries\n",
    "    we select only the text from each dictionary in the abstract\n",
    "    with a length greater than 100\n",
    "    \n",
    "    abstract = [\n",
    "    {\n",
    "        \"text\":\n",
    "    }, \n",
    "    \n",
    "    {\n",
    "        \"text\":\n",
    "    },\n",
    "    \n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    for j in range(len(current_data[\"abstract\"])):\n",
    "        # Selects each dictionary in the abstract list\n",
    "        current_block = current_data[\"abstract\"][j]\n",
    "    \n",
    "        # This selects the text in each block of data\n",
    "        text = current_block[\"text\"] \n",
    "        \n",
    "        # To ensure that random bits of data do not creep in to the code\n",
    "        if len(text) > 200:\n",
    "            temp_ls.append(text)\n",
    "            \n",
    "    # Now we append the list of abstract texts from EACH file, to the abstractls if it is not empty\n",
    "    if len(temp_ls) != 0:\n",
    "        abstractls.append(temp_ls)\n",
    "    \n",
    "    # When we delete it\n",
    "    temp_ls = []\n",
    "    \n",
    "    # Now iterating through the body text of each file\n",
    "    \"\"\"\n",
    "    body_text is a list containing dictionaries\n",
    "    we select only the text from each dictionary in the abstract\n",
    "    with a length greater than 200 chars\n",
    "    \n",
    "    body_text = [\n",
    "    {\n",
    "        \"text\":\n",
    "    }, \n",
    "    \n",
    "    {\n",
    "        \"text\":\n",
    "    },\n",
    "    \n",
    "    ]\n",
    "    \"\"\"\n",
    "    \n",
    "    for k in range(len(current_data[\"body_text\"])):\n",
    "        # Selecting the block in the body text\n",
    "        current_block = current_data[\"body_text\"][k]\n",
    "        \n",
    "        # Selecting the text in the current block\n",
    "        text = current_block[\"text\"]\n",
    "        if len(text) > 200:\n",
    "            temp_ls.append(text)\n",
    "    \n",
    "    if len(temp_ls) != 0:\n",
    "        bodyls.append(temp_ls)\n",
    "        \n",
    "    temp_ls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, we go through both abstractls and the bodyls, convert all the text to lowercase\n",
    "and then use regex to remove numbers and special chars.\n",
    "\n",
    "Then, we convert words to their root forms using, and we ignore parts of speech\n",
    "like the, for, etc, using stopwords\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "CAUTION!!!! PLEASE EXECUTE THE BELOW CODE ONLY IF YOU ABSOLUTELY NEED TO. WE\n",
    "HAVE ALREADY EXECUTED THIS AND THE OUTPUT IS STROED IN SEPARATE.txt FILES.\n",
    "THE FOLLOWING LINES OF CODE TAKE A REALLY LONG TIME TO EXECUTE.\n",
    "\"\"\"\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for i in range(len(bodyls)):\n",
    "    current_list = bodyls[i]\n",
    "    for j in range(len(current_list)):\n",
    "        bodyls[i][j] = bodyls[i][j].lower()\n",
    "        bodyls[i][j] = re.sub('[^a-zA-Z]', ' ', bodyls[i][j])\n",
    "        bodyls[i][j] = bodyls[i][j].split()\n",
    "        bodyls[i][j] = [ps.stem(word) for word in bodyls[i][j] if not word in set(stopwords.words('english'))]\n",
    "        bodyls[i][j] = ' '.join(bodyls[i][j])\n",
    "        \n",
    "for i in range(len(abstractls)):\n",
    "    clist2 = abstractls[i]\n",
    "    for k in range(len(clist2)):\n",
    "        abstractls[i][k] = abstractls[i][k].lower()\n",
    "        abstractls[i][k] = re.sub('[^a-zA-Z]', ' ', abstractls[i][k])\n",
    "        abstractls[i][k] = abstractls[i][k].split()\n",
    "        abstractls[i][k] = [ps.stem(word) for word in abstractls[i][k] if not word in set(stopwords.words('english'))]\n",
    "        abstractls[i][k] = ' '.join(abstractls[i][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we write the data in these variables to a file so as to make the process of reading and writing easier\n",
    "This is done to ensure lesser code execution time\n",
    "\"\"\"\n",
    "\n",
    "with open(\"abstract.txt\", \"w+\") as f:\n",
    "    f.write(str(abstractls))\n",
    "with open(\"body.txt\", \"w+\") as f:\n",
    "    f.write(str(bodyls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic code to try and find the incubation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## have to read from the file to get this done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
